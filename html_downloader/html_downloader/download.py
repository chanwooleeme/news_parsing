# html_downloader/downloader.py

import os
import hashlib
import feedparser
import requests
from typing import Dict, List, Optional
from common.logger import get_logger
from html_downloader.article_store import ArticleStore  # âœ… RedisManagerê°€ ì•„ë‹ˆë¼ ArticleStore ì‚¬ìš©
from common.redis_manager import RedisManager  # ì£¼ì…ë°›ì„ RedisManager

logger = get_logger(__name__)


class HtmlDownloaderConfig:
    def __init__(self, html_dir: str, ttl_seconds: int = 3 * 24 * 3600):
        self.html_dir = html_dir
        self.ttl_seconds = ttl_seconds  # TTL ê¸°ë³¸ê°’ì€ 3ì¼


class HtmlDownloader:
    def __init__(self, config: HtmlDownloaderConfig, redis_manager: RedisManager):
        self.config = config
        self.article_store = ArticleStore(redis_manager)  # âœ… RedisManagerê°€ ì•„ë‹ˆë¼ ArticleStoreë¡œ ê°ìŒˆ

    @staticmethod
    def _generate_filename(url: str) -> str:
        return f"{hashlib.md5(url.encode()).hexdigest()}.html"

    def _save_html(self, publisher: str, url: str, content: bytes) -> str:
        publisher_dir = os.path.join(self.config.html_dir, publisher)
        os.makedirs(publisher_dir, exist_ok=True)

        filename = self._generate_filename(url)
        filepath = os.path.join(publisher_dir, filename)

        with open(filepath, 'wb') as f:
            f.write(content)

        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath

    def _download_html(self, url: str) -> Optional[bytes]:
        try:
            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
            response.raise_for_status()
            return response.content
        except Exception as e:
            logger.warning(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
            return None

    def download_articles(self, rss_links_by_publisher: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        RSS ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ìƒˆë¡œìš´ ê¸°ì‚¬ë§Œ ë‹¤ìš´ë¡œë“œí•˜ê³  ì €ì¥.
        
        Args:
            rss_links_by_publisher (publisher -> [urls])

        Returns:
            ì €ì¥ ì„±ê³µí•œ ê¸°ì‚¬ë“¤ (publisher -> [urls])
        """
        results = {}

        for publisher, urls in rss_links_by_publisher.items():
            logger.info(f"ğŸ“° {publisher} ì²˜ë¦¬ ì‹œì‘ ({len(urls)}ê°œ URL)")

            # âœ… Redisë¡œ í•„í„°ë§: ì•„ì§ ì €ì¥ ì•ˆëœ URLë§Œ
            processed_status = self.article_store.is_articles_processed(urls)
            new_urls = [url for url, processed in processed_status.items() if not processed]

            saved_urls = []
            for url in new_urls:
                content = self._download_html(url)
                if content:
                    self._save_html(publisher, url, content)
                    saved_urls.append(url)

            if saved_urls:
                results[publisher] = saved_urls
                self.article_store.store_articles({publisher: saved_urls})
                logger.info(f"âœ… {publisher} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(saved_urls)}ê°œ)")

        return results

    @staticmethod
    def parse_rss_sources(rss_urls_by_publisher: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        ì™¸ë¶€ì—ì„œ ë°›ì€ RSS ì£¼ì†Œë¥¼ íŒŒì‹±í•´ì„œ ê¸°ì‚¬ ë§í¬ ëª¨ìœ¼ê¸°.
        """
        article_links = {}

        for publisher, feed_urls in rss_urls_by_publisher.items():
            links = set()
            for feed_url in feed_urls:
                try:
                    feed = feedparser.parse(feed_url)
                    links.update(entry.link for entry in feed.entries)
                except Exception as e:
                    logger.warning(f"âŒ RSS íŒŒì‹± ì‹¤íŒ¨: {feed_url}, ì˜¤ë¥˜: {e}")

            article_links[publisher] = list(links)
            logger.info(f"ğŸ“„ {publisher}: {len(links)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

        return article_links

# if __name__ == "__main__":
#     config = HtmlDownloaderConfig(html_dir="html_files")
#     redis_manager = RedisManager()
#     downloader = HtmlDownloader(config, redis_manager)
#     downloader.download_articles(rss_links_by_publisher)
