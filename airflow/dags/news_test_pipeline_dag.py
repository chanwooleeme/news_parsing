from datetime import datetime, timedelta
import logging
import os
import shutil
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# RSS í”¼ë“œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from rss_feed.feed_parser import FeedParser

# indexer ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from indexer import NewsIndexer

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["RSS_FEED_HTML_DIR"] = os.getenv("RSS_FEED_HTML_DIR", "/opt/airflow/data/html")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "")
os.environ["QDRANT_HOST"] = os.getenv("QDRANT_HOST", "")
os.environ["QDRANT_API_KEY"] = os.getenv("QDRANT_API_KEY", "")

# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
TARGET_PUBLISHERS = ["ê²½í–¥ì‹ ë¬¸", "ë‰´ì‹œìŠ¤"]
MAX_HTML_PER_PUBLISHER = 5

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def process_feeds_test():
    """ì§€ì •ëœ ë°œí–‰ì‚¬ì—ì„œ ì œí•œëœ ìˆ˜ì˜ HTML íŒŒì¼ì„ ìˆ˜ì§‘í•˜ëŠ” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    parser = FeedParser()
    parser.process_feeds_for_test(
        target_publishers=TARGET_PUBLISHERS,
        max_html_per_publisher=MAX_HTML_PER_PUBLISHER
    )
    logging.info("í…ŒìŠ¤íŠ¸ RSS í”¼ë“œ ì²˜ë¦¬ ì™„ë£Œ")

def index_articles():
    """HTML íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ìž„ë² ë”©í•˜ì—¬ ì €ìž¥í•©ë‹ˆë‹¤."""
    indexer = NewsIndexer()
    indexer.process_articles()
    logging.info("ìž„ë² ë”© ë° ì €ìž¥ ì™„ë£Œ")

def upload_html_to_s3(html_dir: str, bucket_name: str):
    """HTML íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ"""
    s3 = S3Hook(aws_conn_id="aws_default")
    html_files_uploaded = 0

    for root, _, files in os.walk(html_dir):
        for file in files:
            if not file.endswith(".html"):
                continue

            local_path = os.path.join(root, file)
            relative_path = os.path.relpath(local_path, html_dir)
            s3_key = relative_path.replace("\\", "/")

            try:
                s3.load_file(
                    filename=local_path,
                    key=s3_key,
                    bucket_name=bucket_name,
                    replace=True
                )
                html_files_uploaded += 1
                logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {s3_key}")
            except Exception as e:
                logging.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_key}, ì˜¤ë¥˜: {e}")

    logging.info(f"ðŸ“¦ ì´ {html_files_uploaded}ê°œ HTML íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")

def cleanup_html(html_dir: str):
    """HTML ë””ë ‰í† ë¦¬ ë‚´ë¶€ë§Œ ì •ë¦¬í•˜ê³  ë””ë ‰í† ë¦¬ ìžì²´ëŠ” ìœ ì§€"""
    try:
        if not os.path.exists(html_dir):
            logging.warning(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŒ: {html_dir}")
            return

        # ë””ë ‰í† ë¦¬ ë‚´ë¶€ì˜ ëª¨ë“  í•­ëª© ì‚­ì œ
        for item in os.listdir(html_dir):
            item_path = os.path.join(html_dir, item)
            if os.path.isdir(item_path):
                shutil.rmtree(item_path)
            else:
                os.remove(item_path)

        logging.info(f"ðŸ“¦ HTML ë””ë ‰í† ë¦¬ ë‚´ë¶€ ì •ë¦¬ ì™„ë£Œ: {html_dir}")
    except Exception as e:
        logging.error(f"âŒ HTML ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

with DAG(
    'news_pipeline_dag_test',
    default_args=default_args,
    description='ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸',
    schedule_interval=timedelta(hours=1),
    catchup=False,
    tags=['rss', 'html', 'parser', 'news', 'embedding', 'test'],
) as dag:

    process_feeds_task = PythonOperator(
        task_id='process_feeds',
        python_callable=process_feeds_test,
    )
    
    upload_html_task = PythonOperator(
        task_id='upload_html_to_s3',
        python_callable=upload_html_to_s3,
        op_kwargs={
            'html_dir': os.environ["RSS_FEED_HTML_DIR"],
            'bucket_name': os.environ["AWS_S3_BUCKET"]
        }
    )

    index_articles_task = PythonOperator(
        task_id='index_articles',
        python_callable=index_articles,
    )

    cleanup_html_task = PythonOperator(
        task_id='cleanup_html',
        python_callable=cleanup_html,
        op_kwargs={
            'html_dir': os.environ["RSS_FEED_HTML_DIR"]
        }
    )

    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    process_feeds_task >> upload_html_task >> index_articles_task >> cleanup_html_task
    