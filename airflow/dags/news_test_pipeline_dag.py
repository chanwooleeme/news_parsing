from datetime import datetime, timedelta
import logging
import os
from typing import Dict, Any
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator


from airflow.decorators import dag, task
from airflow import DAG
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# RSS í”¼ë“œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from rss_feed.feed_parser import FeedParser

# indexer ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from indexer import NewsIndexer

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["RSS_FEED_HTML_DIR"] = os.getenv("RSS_FEED_HTML_DIR", "/opt/airflow/data/html")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "")
os.environ["QDRANT_HOST"] = os.getenv("QDRANT_HOST", "")
os.environ["QDRANT_API_KEY"] = os.getenv("QDRANT_API_KEY", "")

# DAG ê¸°ë³¸ íŒŒë¼ë¯¸í„°
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
TARGET_PUBLISHERS = ["ê²½í–¥ì‹ ë¬¸", "ë‰´ì‹œìŠ¤"]
MAX_HTML_PER_PUBLISHER = 5
EXCLUDED_NEWSPAPERS = []  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ì œì™¸ ì‹ ë¬¸ì‚¬ ì—†ìŒ


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def process_feeds_test() -> Dict[str, Any]:
    """ì§€ì •ëœ ë°œí–‰ì‚¬ì—ì„œ ì œí•œëœ ìˆ˜ì˜ HTML íŒŒì¼ì„ ìˆ˜ì§‘í•˜ëŠ” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    parser = FeedParser()
    parser.process_feeds_for_test(
        target_publishers=TARGET_PUBLISHERS,
        max_html_per_publisher=MAX_HTML_PER_PUBLISHER
    )
    logging.info(f"í…ŒìŠ¤íŠ¸ RSS í”¼ë“œ ì²˜ë¦¬ ì™„ë£Œ")


def index_articles():
    """HTML íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ì„ë² ë”©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    # indexer íŒ¨í‚¤ì§€ì˜ ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ì‚¬ìš©
    indexer = NewsIndexer()
    indexer.process_articles()
    logging.info("ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ")


def upload_and_cleanup_html(html_dir: str, bucket_name: str):
    print(f"[DEBUG] HTML í´ë”ì— ìˆëŠ” íŒŒì¼ í™•ì¸: {html_dir}")
    for root, _, files in os.walk(html_dir):
        for file in files:
            print("    - ", os.path.join(root, file))
            
    s3 = S3Hook(aws_conn_id="aws_default")
    html_files_uploaded = 0

    for root, _, files in os.walk(html_dir):
        for file in files:
            if not file.endswith(".html"):
                continue

            local_path = os.path.join(root, file)
            # ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ S3 key ì„¤ì • (ì˜ˆ: ê²½í–¥ì‹ ë¬¸/abc123.html)
            relative_path = os.path.relpath(local_path, html_dir)
            s3_key = relative_path.replace("\\", "/")  # for Windows compatibility

            try:
                s3.load_file(
                    filename=local_path,
                    key=s3_key,
                    bucket_name=bucket_name,
                    replace=True
                )
                os.remove(local_path)  # ì—…ë¡œë“œ ì„±ê³µ ì‹œ ì‚­ì œ
                html_files_uploaded += 1
                print(f"âœ… ì—…ë¡œë“œ ë° ì‚­ì œ ì™„ë£Œ: {s3_key}")
            except Exception as e:
                print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_key}, ì˜¤ë¥˜: {e}")

    print(f"ğŸ“¦ ì´ {html_files_uploaded}ê°œ HTML íŒŒì¼ ì—…ë¡œë“œ ë° ì •ë¦¬ ì™„ë£Œ")

    

with DAG(
    'news_pipeline_dag_test',
    default_args=default_args,
    description='ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸',
    schedule_interval=timedelta(hours=1),
    catchup=False,
    tags=['rss', 'html', 'parser', 'news', 'embedding'],
) as dag:

    # RSS í”¼ë“œ ì²˜ë¦¬ íƒœìŠ¤í¬
    process_feeds_task = PythonOperator(
        task_id='process_feeds',
        python_callable=process_feeds_test,
    )
    
    index_articles_task = PythonOperator(
        task_id='index_articles',
        python_callable=index_articles,
    )
    
    upload_task = PythonOperator(
        task_id='upload_and_cleanup_html',
        python_callable=upload_and_cleanup_html,
        op_kwargs={
            'html_dir': os.environ["RSS_FEED_HTML_DIR"],
            'bucket_name': os.environ["AWS_S3_BUCKET"]
        }
    )


    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    process_feeds_task >> index_articles_task >> upload_task
    