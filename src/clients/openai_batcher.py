import tiktoken
from logger import get_logger
from openai import OpenAI
from agentic_retriever.config.models import ModelName
from typing import List

logger = get_logger(__name__)

class OpenAIBatcher:
    def __init__(self, openai_client: OpenAI, model: ModelName, max_tokens=128000, margin=2000):
        self.client = openai_client
        self.model = model
        self.max_tokens = max_tokens
        self.margin = margin
        self.encoding = tiktoken.encoding_for_model(self.model.name)
        logger.info(f"ğŸ”„ OpenAIBatcher ì´ˆê¸°í™” ì™„ë£Œ (model: {model.name}, max_tokens: {max_tokens}, margin: {margin})")

    def _count_tokens(self, text: str) -> int:
        token_count = len(self.encoding.encode(text))
        logger.debug(f"ğŸ“Š í† í° ìˆ˜ ê³„ì‚°: {token_count} tokens")
        return token_count

    def _smart_split(self, text: str) -> List[str]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ í† í° í•œë„ì— ë§ê²Œ ë¶„í• """
        logger.info("âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í•  ì‹œì‘")
        lines = text.split('\n')
        batches, current_batch, current_tokens = [], [], 0

        for line in lines:
            line_tokens = self._count_tokens(line)
            if current_tokens + line_tokens > (self.max_tokens - self.margin):
                batches.append("\n".join(current_batch))
                current_batch, current_tokens = [], 0
            current_batch.append(line)
            current_tokens += line_tokens

        if current_batch:
            batches.append("\n".join(current_batch))

        logger.info(f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(batches)} ë°°ì¹˜ ìƒì„±")
        return batches

    def _optimize_batch_size(self, texts: List[str]) -> List[List[str]]:
        """í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ”"""
        batches, current_batch, current_tokens = [], [], 0
        for text in texts:
            token_count = self._count_tokens(text)
            if current_tokens + token_count <= (self.max_tokens - self.margin):
                current_batch.append(text)
                current_tokens += token_count
            else:
                if current_batch:
                    batches.append(current_batch)
                current_batch = [text]
                current_tokens = token_count
        if current_batch:
            batches.append(current_batch)
        return batches

    def batch_chat_completion(self, prompts: List[str], system_prompt: str = "") -> List[str]:
        """ë°°ì¹˜ë¡œ ChatCompletion ìš”ì²­ (í† í° ì´ˆê³¼ì‹œ smart split í¬í•¨)"""
        logger.info(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(prompts)} ê°œì˜ í”„ë¡¬í”„íŠ¸")
        all_responses = []

        for i, prompt in enumerate(prompts):
            token_count = self._count_tokens(prompt)
            if token_count > (self.max_tokens - self.margin):
                logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ {i+1} í† í° ì´ˆê³¼ ({token_count} tokens). smart split ì§„í–‰")
                for part in self._smart_split(prompt):
                    all_responses.append(self._single_chat_completion(part, system_prompt))
            else:
                all_responses.append(self._single_chat_completion(prompt, system_prompt))

        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(all_responses)} ê°œì˜ ì‘ë‹µ ìƒì„±")
        return all_responses

    def _single_chat_completion(self, prompt: str, system_prompt: str) -> str:
        """ê°œë³„ ChatCompletion í˜¸ì¶œ"""
        logger.debug("ğŸ”„ OpenAI Chat API í˜¸ì¶œ ì‹œì‘")
        response = self.client.chat.completions.create(
            model=self.model.name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )
        return response.choices[0].message.content.strip()

    def batch_embed(self, texts: List[str]) -> List[List[float]]:
        """ë°°ì¹˜ë¡œ ì„ë² ë”© ì²˜ë¦¬"""
        if not self.model.is_embedding_model:
            raise ValueError(f"âŒ í˜„ì¬ ëª¨ë¸({self.model.name})ì€ ì„ë² ë”© ì „ìš©ì´ ì•„ë‹™ë‹ˆë‹¤.")

        logger.info(f"ğŸ”„ ì„ë² ë”© ì‹œì‘: {len(texts)} ê°œ í…ìŠ¤íŠ¸")
        all_embeddings = []

        # batch optimize
        batches = self._optimize_batch_size(texts)
        logger.info(f"ğŸ“¦ ì´ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ë¨")

        for i, batch in enumerate(batches):
            try:
                response = self.client.embeddings.create(
                    model=self.model.name,
                    input=batch
                )
                embeddings = [item["embedding"] for item in response.data]
                all_embeddings.extend(embeddings)
                logger.info(f"âœ… ë°°ì¹˜ {i+1}/{len(batches)} ì²˜ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ ë°°ì¹˜ {i+1} ì‹¤íŒ¨ â†’ ê°œë³„ ì„ë² ë”© ì‹œë„: {e}")
                for text in batch:
                    try:
                        response = self.client.embeddings.create(
                            model=self.model.name,
                            input=[text]
                        )
                        embedding = response.data[0]["embedding"]
                        all_embeddings.append(embedding)
                    except Exception as inner_e:
                        logger.error(f"âŒ ë‹¨ì¼ ì„ë² ë”© ì‹¤íŒ¨: {inner_e}")
                        all_embeddings.append([])  # fallback: ë¹ˆ ë²¡í„°

        return all_embeddings
