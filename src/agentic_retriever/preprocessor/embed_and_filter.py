import os
import sys
from typing import List, Dict, Any, Tuple
import json
import logging
from qdrant_client import QdrantClient
from qdrant_client.http import models
from openai import OpenAI
from clients.qdrant_vector_store import QdrantVectorStore
from utils.client import get_qdrant_client, get_openai_client
from utils.file import list_directories, list_files, join_path, read_json_file

# ë¡œê±° ì„¤ì • (logger.py ëª¨ë“ˆ ì‚¬ìš© ì‹œ ëŒ€ì²´ ê°€ëŠ¥)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    ch.setFormatter(formatter)
    logger.addHandler(ch)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒìˆ˜: ê²½ì œ/ë¹„ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ í…Œì´ë¸” ë° GOOD_QUERIES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ECONOMY_KEYWORD_WEIGHTS = {
    "ê²½ì œ": 1.0, "ê¸ˆìœµ": 1.0, "ì£¼ì‹": 1.0, "ë¶€ë™ì‚°": 1.0, "ê¸ˆë¦¬": 1.0,
    "ì¸í”Œë ˆì´ì…˜": 1.0, "GDP": 1.0, "ìˆ˜ì¶œ": 1.0, "ìˆ˜ì…": 1.0,
    "ê¸°ì—…": 0.8, "ì‹œì¥": 0.8, "íˆ¬ì": 0.8, "ê³ ìš©": 0.8, "ì†Œë¹„": 0.8,
    "ë¬¼ê°€": 0.8, "í™˜ìœ¨": 0.8, "ì±„ê¶Œ": 0.8, "ì¦ê¶Œ": 0.8, "ì€í–‰": 0.8, "ê¸ˆìœµê¶Œ": 0.8,
}
NON_ECONOMY_KEYWORD_PENALTIES = {
    "ì •ì¹˜": -0.5, "ì™¸êµ": -0.5, "ì‚¬íšŒ": -0.5, "ë¬¸í™”": -0.5,
    "ìŠ¤í¬ì¸ ": -0.5, "ì—°ì˜ˆ": -0.5, "êµìœ¡": -0.5, "ê³¼í•™": -0.5,
    "ê¸°ìˆ ": -0.3, "í™˜ê²½": -0.3, "ì˜ë£Œ": -0.3, "ë²•ë¥ ": -0.3,
}

GOOD_QUERIES = [
    "ë¯¸êµ­ ì—°ì¤€ì˜ ê¸ˆë¦¬ ê²°ì •",
    "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ìƒìŠ¹ë¥ ",
    "ì›ìì¬ ê°€ê²© ë³€ë™",
    "ìƒê°€ ì„ëŒ€ë£Œ ë™í–¥",
    "ì½”ìŠ¤í”¼ ê¸‰ë½ ì´ìœ ",
    "ê¸°ì—… ì‹¤ì  ì˜ˆì¸¡",
    "ë¶€ë™ì‚° ê°€ê²© ë™í–¥",
    "ë¬´ì—­ìˆ˜ì§€ ì ì ì›ì¸",
    "ì‹¤ì—…ë¥  ì¦ê°€ ë°°ê²½",
    "ì—ë„ˆì§€ ê°€ê²© ìƒìŠ¹ ê¸°ì‚¬",
    "ì½”ìŠ¤ë‹¥ íˆ¬ìì ë™í–¥",
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize(value: float, min_val: float, max_val: float) -> float:
    if max_val == min_val:
        return 0.0
    return max(0.0, min(1.0, (value - min_val) / (max_val - min_val)))

def calculate_keyword_score(text: str) -> float:
    score = 0.0
    text = text.lower()
    for keyword, weight in ECONOMY_KEYWORD_WEIGHTS.items():
        if keyword.lower() in text:
            score += weight
    for keyword, penalty in NON_ECONOMY_KEYWORD_PENALTIES.items():
        if keyword.lower() in text:
            score += penalty
    return score

def create_embeddings_batch(client: OpenAI, texts: List[str], model: str, batch_size: int = 10) -> List[List[float]]:
    """í…ìŠ¤íŠ¸ ë°°ì¹˜ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = client.embeddings.create(
            model=model,
            input=batch
        )
        batch_embeddings = [item.embedding for item in response.data]
        embeddings.extend(batch_embeddings)
        logger.info(f"âœ… {min(i+batch_size, len(texts))}/{len(texts)}ê°œ ì„ë² ë”© ì™„ë£Œ")
    return embeddings

def load_articles(file_path: str) -> List[Dict[str, Any]]:
    """JSON íŒŒì¼ì—ì„œ ê¸°ì‚¬ ë¡œë“œ"""
    with open(os.path.join(file_path), 'r', encoding='utf-8') as f:
        return json.load(f)

def create_collection_if_not_exists(q_client: QdrantClient, name: str):
    try:
        q_client.get_collection(collection_name=name)
        logger.info(f"ğŸ“¦ ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸ë¨: {name}")
    except Exception as e:
        logger.info(f"ğŸ†• ì»¬ë ‰ì…˜ ìƒì„± ì‹œë„: {name}")
        q_client.create_collection(
            collection_name=name,
            vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)
        )
        logger.info(f"ğŸ†• ì»¬ë ‰ì…˜ ìƒì„±ë¨: {name}")

def hybrid_search_with_score(
    vector_store,
    query: str,
    query_embedding: List[float],
    articles_lookup: Dict[str, Dict],
    top_k: int = 5
) -> List[Dict]:
    """
    ì„ë² ë”© ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë¥¼ ê²°í•©í•´ ìµœì¢… ì ìˆ˜(final_score)ë¥¼ ì‚°ì¶œí•œ í›„,
    ìµœì¢… ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ top_k ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    articles_lookup: { custom_id: article } í˜•íƒœë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    results = vector_store.search(
        query_vector=query_embedding,
        top_k=top_k * 2  # í›„ë³´êµ° í™•ì¥
    )
    scored_results = []
    for result in results:
        doc_id = result['payload']['custom_id']  # "json_order" ëŒ€ì‹  "custom_id" ì‚¬ìš©
        article = articles_lookup.get(doc_id)
        if not article:
            continue
        keyword_score = max(0.0, calculate_keyword_score(article['content']))
        combined_score = result['score'] * (1 + keyword_score)
        result['keyword_score'] = keyword_score
        result['combined_score'] = combined_score
        result['custom_id'] = doc_id
        scored_results.append(result)
    scores = [r['combined_score'] for r in scored_results]
    if scores:
        min_score, max_score = min(scores), max(scores)
        for r in scored_results:
            r['final_score'] = normalize(r['combined_score'], min_score, max_score)
    else:
        for r in scored_results:
            r['final_score'] = 0.0
    scored_results.sort(key=lambda x: x['final_score'], reverse=True)
    return scored_results[:top_k]

def embed_articles(openai_client: OpenAI, articles: List[Dict]) -> List[List[float]]:
    """ê¸°ì‚¬ë“¤ì„ ì„ë² ë”©í•©ë‹ˆë‹¤."""
    logger.info("ğŸ” Articles ì„ë² ë”© ì‹œì‘...")
    texts = [article['content'] for article in articles]
    embeddings = create_embeddings_batch(openai_client, texts, model="text-embedding-3-small")
    logger.info(f"âœ… Articles ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ")
    return embeddings

def save_embeddings_to_qdrant(
    qdrant_client: QdrantClient,
    articles: List[Dict],
    embeddings: List[List[float]],
    collection_name: str
):
    """ì„ë² ë”©ì„ Qdrantì— ì €ì¥í•©ë‹ˆë‹¤."""
    points = []
    for article, embedding in zip(articles, embeddings):
        doc_id = article["custom_id"]
        points.append(
            models.PointStruct(
                id=doc_id,
                vector=embedding,
                payload={
                    "custom_id": doc_id,
                    "title": article.get("title"),
                    "category": article.get("category"),
                    "publication_date": article.get("publication_date"),
                    "keyword_score": calculate_keyword_score(article['content'])
                }
            )
        )
    batch_size = 25
    for i in range(0, len(points), batch_size):
        qdrant_client.upsert(collection_name=collection_name, points=points[i:i+batch_size])
        logger.info(f"âœ… {collection_name} ì—…ì„œíŠ¸ ì§„í–‰: {min(i+batch_size, len(points))}/{len(points)}")
    logger.info(f"âœ… {collection_name} ì—…ì„œíŠ¸ ì™„ë£Œ")

def analyze_search_results(results: List[Dict], articles_lookup: Dict[Any, Dict]) -> Dict:
    """
    ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ í•¨ìˆ˜.
    ê° ê²€ìƒ‰ ê²°ê³¼ì˜ 'custom_id'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
    ì‹¤ì œ ê¸°ì‚¬ ë°ì´í„°(articles_lookup)ì˜ 'classification' í•„ë“œë¥¼ í™•ì¸í•˜ì—¬
    ê²½ì œê´€ë ¨("ê²½ì œê´€ë ¨")ì´ë©´ economic, ì•„ë‹ˆë©´ non-economicìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    analysis = {
        'total_found': len(results),
        'economic_found': 0,
        'non_economic_found': 0,
        'economic_indices': [],
        'non_economic_indices': []
    }
    for result in results:
        doc_id = result['payload']['custom_id']
        article = articles_lookup.get(doc_id, {})
        if article.get('classification') == 'ê²½ì œê´€ë ¨':
            analysis['economic_found'] += 1
            analysis['economic_indices'].append(doc_id)
        else:
            analysis['non_economic_found'] += 1
            analysis['non_economic_indices'].append(doc_id)
    return analysis

def query_articles(
    qdrant_client: QdrantClient,
    openai_client: OpenAI,
    articles_lookup: Dict[str, Dict],
    collection_name: str
) -> Tuple[List[Dict], List[Dict]]:
    """í‚¤ì›Œë“œ ì ìˆ˜ì™€ ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ ê²½ì œ/ë¹„ê²½ì œ ê¸°ì‚¬ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    logger.info(f"ğŸ” ê¸°ì‚¬ ë¶„ë¥˜ ì‹œì‘ (ì´ {len(articles_lookup)}ê°œ ê¸°ì‚¬)")
    
    # 1. ë²¡í„° ê²€ìƒ‰ (ì´ì „ ì½”ë“œ ìœ ì§€)
    query_embeddings = create_embeddings_batch(openai_client, GOOD_QUERIES, model="text-embedding-3-small")
    article_scores = {doc_id: [] for doc_id in articles_lookup.keys()}
    vector_store = QdrantVectorStore(qdrant_client, collection_name)
    
    for i, query in enumerate(GOOD_QUERIES):
        logger.info(f"ğŸ” ì¿¼ë¦¬ ê²€ìƒ‰: {query}")
        results = hybrid_search_with_score(vector_store, query, query_embeddings[i], articles_lookup, top_k=50)
        for result in results:
            doc_id = result['custom_id']
            score = result['final_score']
            article_scores[doc_id].append(score)
    
    # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ìµœì¢… ë°©ì‹)
    economy_points = []
    others_points = []
    vector_hits = set(doc_id for doc_id, scores in article_scores.items() if scores)
    keyword_matches = set()
    
    for doc_id, article in articles_lookup.items():
        content = article.get('content', '')
        keyword_score = calculate_keyword_score(content)
        
        # í‚¤ì›Œë“œ ì ìˆ˜ê°€ 0.5 ì´ìƒì´ë©´ ê²½ì œ ê´€ë ¨ìœ¼ë¡œ ë¶„ë¥˜
        is_economic = keyword_score > 0.5
        if is_economic:
            keyword_matches.add(doc_id)
            
        is_vector_hit = doc_id in vector_hits
        
        point = {
            "custom_id": doc_id,
            "title": article.get("title", ""),
            "keyword_score": keyword_score,
            "vector_hit": is_vector_hit,
            "is_economic": is_economic
        }
        
        if is_economic:
            economy_points.append(point)
        else:
            others_points.append(point)
    
    logger.info(f"ğŸ“Š ìµœì¢… ë¶„ë¥˜ ê²°ê³¼:")
    logger.info(f"- ê²½ì œê´€ë ¨ ê¸°ì‚¬: {len(economy_points)}ê°œ")
    logger.info(f"- ë¹„ê²½ì œê´€ë ¨ ê¸°ì‚¬: {len(others_points)}ê°œ")
    logger.info(f"- ë²¡í„° ê²€ìƒ‰ íˆíŠ¸: {len(vector_hits)}ê°œ")
    logger.info(f"- í‚¤ì›Œë“œ ë§¤ì¹˜: {len(keyword_matches)}ê°œ")
    logger.info(f"- ë²¡í„°ë§Œ ë§¤ì¹˜: {len(vector_hits - keyword_matches)}ê°œ")
    logger.info(f"- í‚¤ì›Œë“œë§Œ ë§¤ì¹˜: {len(keyword_matches - vector_hits)}ê°œ")
    logger.info(f"- ëª¨ë‘ ë§¤ì¹˜: {len(vector_hits & keyword_matches)}ê°œ")
    
    return economy_points, others_points

def run_filter_pipeline(
    qdrant_client: QdrantClient,
    openai_client: OpenAI,
    articles: List[Dict],
    collection_economy: str,
    collection_others: str,
    collection_all: str,
):
    """
    ì „ì²´ í•„í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜.
      1. ê¸°ì‚¬ ë°ì´í„°ë¥¼ custom_id ê¸°ì¤€ lookup ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
      2. GOOD_QUERIES ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ì„ í†µí•´, ê²€ìƒ‰ ê²°ê³¼ì— í•œ ë²ˆì´ë¼ë„ ë“±ì¥í•œ ë¬¸ì„œë¥¼
         ì˜ˆì¸¡ ê²½ì œ ê¸°ì‚¬(economy)ë¡œ ë¶„ë¥˜í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ê¸°íƒ€(others)ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
      3. ê° ë¬¸ì„œë³„ í‰ê·  ìŠ¤ì½”ì–´(mean_score)ë¥¼ ê³„ì‚°í•˜ì—¬ ë©”íƒ€ë°ì´í„°ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Returns:
        Dict: í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("ğŸš€ í•„í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info(f"ğŸ“Š ì²˜ë¦¬í•  ì´ ê¸°ì‚¬ ìˆ˜: {len(articles)}")
    
    # articles_lookup ìƒì„± (í‚¤: custom_id)
    articles_lookup = { article["custom_id"]: article for article in articles }
    logger.info(f"âœ… ê¸°ì‚¬ lookup ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ: {len(articles_lookup)}ê°œ")
    
    # GOOD_QUERIES ê¸°ë°˜ ë¬¸ì„œ ë¶„ë¥˜ (ì´ë¯¸ ì €ì¥ëœ collection_allì„ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰)
    economy_points, others_points = query_articles(
        qdrant_client,
        openai_client,
        articles_lookup,
        collection_all
    )
    
    # ì •í™•ë„ í‰ê°€ (economy_pointsì˜ ê° í•­ëª©ì´ ì‹¤ì œë¡œ "ê²½ì œ" ì¹´í…Œê³ ë¦¬ì¸ì§€ í™•ì¸)
    success_count = 0
    fail_count = 0
    
    for point in economy_points:
        custom_id = point["custom_id"]
        # articles_lookupì—ì„œ í•´ë‹¹ IDì˜ ê¸°ì‚¬ ì°¾ê¸°
        article = articles_lookup.get(custom_id)
        if article and article.get("category") == "ê²½ì œ":
            success_count += 1
        else:
            fail_count += 1
    
    total_count = success_count + fail_count
    accuracy = (success_count / total_count * 100) if total_count > 0 else 0
    
    # ê²½ì œì¸ë° ë¹„ê²½ì œë¡œ ì˜ëª» ë¶„ë¥˜ëœ í•­ëª©(false negative) í‰ê°€ ì¶”ê°€
    false_negative_count = 0
    false_negatives = []
    total_economy_articles = 0
    
    # ì „ì²´ ë°ì´í„°ì—ì„œ ì‹¤ì œ ê²½ì œ ê¸°ì‚¬ ìˆ˜ í™•ì¸
    for article in articles:
        if article.get("category") == "ê²½ì œ":
            total_economy_articles += 1
    
    # others_pointsì—ì„œ ì‹¤ì œë¡œëŠ” ê²½ì œ ê¸°ì‚¬ì¸ í•­ëª© í™•ì¸ (false negative)
    for point in others_points:
        custom_id = point["custom_id"]
        article = articles_lookup.get(custom_id)
        if article and article.get("category") == "ê²½ì œ":
            false_negative_count += 1
            false_negatives.append({
                "custom_id": custom_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "keyword_score": point.get("keyword_score", 0)
            })
    
    false_negative_rate = (false_negative_count / total_economy_articles * 100) if total_economy_articles > 0 else 0
    recall = ((total_economy_articles - false_negative_count) / total_economy_articles * 100) if total_economy_articles > 0 else 0
    
    logger.info("ğŸ“Š ê²½ì œ ë¶„ë¥˜ ì •í™•ë„ í‰ê°€:")
    logger.info(f"- ì´ ì˜ˆì¸¡ ê²½ì œ ê¸°ì‚¬: {total_count}ê°œ")
    logger.info(f"- ì‹¤ì œ ê²½ì œ ê¸°ì‚¬(ì„±ê³µ): {success_count}ê°œ")
    logger.info(f"- ë¹„ê²½ì œ ê¸°ì‚¬(ì‹¤íŒ¨): {fail_count}ê°œ")
    logger.info(f"- ë¶„ë¥˜ ì •í™•ë„: {accuracy:.2f}%")
    logger.info(f"- ì‹¤ì œ ê²½ì œ ê¸°ì‚¬ ì´ ìˆ˜: {total_economy_articles}ê°œ")
    logger.info(f"- ê²½ì œì¸ë° ë¹„ê²½ì œë¡œ ì˜¤ë¶„ë¥˜(False Negative): {false_negative_count}ê°œ")
    logger.info(f"- False Negative ë¹„ìœ¨: {false_negative_rate:.2f}%")
    logger.info(f"- ê²½ì œ ê¸°ì‚¬ ê²€ì¶œë¥ (Recall): {recall:.2f}%")
    
    # ê²½ì œì¸ë° ë¹„ê²½ì œë¡œ ì˜¤ë¶„ë¥˜ëœ ì‚¬ë¡€ë“¤ ì¶œë ¥
    logger.info("\nğŸ“‹ ê²½ì œì¸ë° ë¹„ê²½ì œë¡œ ì˜¤ë¶„ë¥˜ëœ ì‚¬ë¡€:")
    for i, fn in enumerate(false_negatives, 1):
        logger.info(f"{i}. ID: {fn['custom_id']}, í‚¤ì›Œë“œ ì ìˆ˜: {fn['keyword_score']:.3f}")
        logger.info(f"   ì œëª©: {fn['title'][:100]}{'...' if len(fn['title']) > 100 else ''}\n")
    
    logger.info("ğŸ‰ Filter pipeline completed successfully.")
    
    # ê²°ê³¼ ë°˜í™˜
    return {
        "total_articles": len(articles),
        "total_economy_articles": total_economy_articles,
        "predicted_economy": total_count,
        "true_positive": success_count,
        "false_positive": fail_count,
        "false_negative": false_negative_count,
        "accuracy": accuracy,
        "false_negative_rate": false_negative_rate,
        "recall": recall,
        "precision": (success_count / total_count * 100) if total_count > 0 else 0,
        "f1_score": (2 * recall * (success_count / total_count * 100) / (recall + (success_count / total_count * 100))) if (recall + (success_count / total_count * 100)) > 0 else 0,
        "false_negatives": false_negatives
    }

if __name__ == "__main__":
    qdrant_client = get_qdrant_client()
    openai_client = get_openai_client()
    articles = load_articles('/Users/lee/Desktop/news_parsing/test/llm_output.json')
    
    if articles:
        # í•„ìš”í•œ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        create_collection_if_not_exists(qdrant_client, "collection_all6")
        create_collection_if_not_exists(qdrant_client, "collection_economy6")
        create_collection_if_not_exists(qdrant_client, "collection_others6")
        
        # 1. ë¨¼ì € ê¸°ì‚¬ ì„ë² ë”© ë° Qdrant ì €ì¥ (ë¼ë²¨ê³¼ ë¬´ê´€í•˜ê²Œ)
        embeddings = embed_articles(openai_client, articles)
        save_embeddings_to_qdrant(qdrant_client, articles, embeddings, "collection_all6")
        
        # 2. GOOD_QUERIESë¥¼ í†µí•œ ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê²½ì œ ê´€ë ¨ ë¬¸ì„œë¥¼ í›„ë³´ë¡œ ë½‘ê³ ,
        #    ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ë¬¸ì„œì˜ í‰ê·  ìŠ¤ì½”ì–´(mean_score)ë¥¼ ê³„ì‚°í•˜ì—¬ í‰ê°€
        run_filter_pipeline(qdrant_client, openai_client, articles, "collection_economy6", "collection_others6", "collection_all6")
    else:
        logger.error("âŒ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
