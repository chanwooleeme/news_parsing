from .base_parser import BaseParser
import re
import json
import logging
from typing import Tuple, Optional
from datetime import datetime
import html
class ChosunParser(BaseParser):
    def __init__(self, html: str):
        super().__init__(html)
        self._processed_data = None  # Fusion ë°ì´í„° ìºì‹±

    def get_author(self) -> str:
        """BaseParserì˜ get_author ì˜¤ë²„ë¼ì´ë“œ"""
        try:
            processed = self._process_fusion_data()
            if processed and processed[1]:
                return self._clean_author(processed[1])
            
            # Fusion ë°ì´í„°ì—ì„œ ì‘ì„±ìë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ëŒ€ì•ˆì  ë°©ë²• ì‹œë„
            byline = self.soup.select_one(".byline")
            if byline:
                return self._clean_author(byline.get_text(strip=True))
                
            return ""
        except Exception as e:
            logging.error(f"ì‘ì„±ì ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""

    def get_content(self) -> str:
        """BaseParserì˜ get_content ì˜¤ë²„ë¼ì´ë“œ"""
        try:
            processed = self._process_fusion_data()
            if processed and processed[0]:
                return self._clean_body(processed[0])
            
            # Fusion ë°ì´í„°ì—ì„œ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ëŒ€ì•ˆì  ë°©ë²• ì‹œë„
            content_container = self.soup.select_one("div.article-body")
            if content_container:
                paragraphs = content_container.select("p")
                if paragraphs:
                    content = " ".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
                    return self.clean_article_text(self._clean_body(content))
            
            return ""
        except Exception as e:
            logging.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""

    def clean_article_text(text: str) -> str:
        # 1. HTML íƒœê·¸ ì œê±° (ì˜ˆ: <b>, <i> ë“±)
        text = re.sub(r"<[^>]+>", "", text)
        
        # 2. HTML ì—”í‹°í‹° ë³€í™˜ (ì˜ˆ: &amp; -> &)
        text = html.unescape(text)
        
        # 3. ëŒ€ê´„í˜¸ë¡œ ê°ì‹¸ì§„ ë©”íƒ€ ì •ë³´ ì œê±° (ì˜ˆ: [í¸ì§‘ì ì£¼], [ë•…ì§‘ê³ ])
        text = re.sub(r"\[[^\]]+\]", "", text)
        
        # 4. í…ìŠ¤íŠ¸ ì–‘ìª½ì— ë¶™ì€ ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œ ë° ê³µë°± ì œê±°
        text = text.strip().strip('\"').strip()
        
        # 5. ë‚´ë¶€ì˜ ì—¬ë¶„ì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
        text = re.sub(r"\s+", " ", text)
        
        return text


    def get_publication_date(self) -> str:
        try:
            iso_str = self.extract_meta("article:published_time")
            if not iso_str:
                # ë‹¤ë¥¸ ë©”íƒ€ íƒœê·¸ ì‹œë„
                iso_str = self.extract_meta("og:published_time") or self.extract_meta("pubdate")
                
            if not iso_str:
                # HTMLì—ì„œ ë°œí–‰ì¼ ì°¾ê¸° ì‹œë„
                date_elem = self.soup.select_one(".publication-date")
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    # ì—¬ê¸°ì„œ ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•˜ì§€ë§Œ, ê°„ë‹¨íˆ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                    return datetime.now().timestamp()
                
                return datetime.now().timestamp()
                
            # 'Z' íƒ€ì„ì¡´ì„ í‘œì¤€ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            iso_str = iso_str.replace("Z", "+00:00")
            dt = datetime.fromisoformat(iso_str)
            unix_timestamp = dt.timestamp()  # ì´ˆ ë‹¨ìœ„ timestamp
            return unix_timestamp
        except Exception as e:
            logging.error(f"ë°œí–‰ì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return datetime.now().timestamp()
    
    def _process_fusion_data(self) -> Optional[Tuple[str, str]]:
        """Fusion ë°ì´í„° ì²˜ë¦¬ (ìºì‹± í¬í•¨)"""
        try:
            if self._processed_data is None:
                script_text = self._find_fusion_script()
                if not script_text:
                    return None
                self._processed_data = self._parse_fusion_script(script_text)
            return self._processed_data
        except Exception as e:
            logging.error(f"Fusion ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None

    def _find_fusion_script(self) -> Optional[str]:
        """Fusion.globalContent ìŠ¤í¬ë¦½íŠ¸ ì°¾ê¸°"""
        try:
            # CSS ì„ íƒìë¡œ ìŠ¤í¬ë¦½íŠ¸ ì°¾ê¸°
            script = self.soup.select_one('script#fusion-metadata')
            if script:
                return script.get_text(strip=True)
                
            # ëŒ€ì•ˆ: ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ìˆœíšŒ
            for script in self.soup.select("script"):
                if script.get("id") == "fusion-metadata" and script.get("type") == "application/javascript":
                    return script.get_text(strip=True)
                
                # ë‚´ìš©ì—ì„œ Fusion.globalContent íŒ¨í„´ ì°¾ê¸°
                if "Fusion.globalContent" in script.get_text():
                    return script.get_text(strip=True)
                    
            return None
        except Exception as e:
            logging.error(f"Fusion ìŠ¤í¬ë¦½íŠ¸ ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return None

    def _parse_fusion_script(self, script_text: str) -> Tuple[str, str]:
        """ìŠ¤í¬ë¦½íŠ¸ íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ"""
        if not script_text:
            return ("", "")
            
        try:
            match = re.search(r'Fusion\.globalContent\s*=\s*({.*?});', script_text, re.DOTALL)
            if not match:
                return ("", "")
            
            data = json.loads(match.group(1))
            content = self._extract_article_content(data)
            author = self._extract_raw_author(data)
            return (content, author)
        except (json.JSONDecodeError, re.error) as e:
            logging.error(f"Fusion ìŠ¤í¬ë¦½íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return ("", "")
        except Exception as e:
            logging.error(f"Fusion ë°ì´í„° ì¶”ì¶œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return ("", "")

    def _extract_article_content(self, data: dict) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            body_parts = [
                el["content"].strip()
                for el in data.get("content_elements", [])
                if el.get("type") == "text" and "content" in el
            ]
            return " ".join(body_parts)
        except Exception as e:
            logging.error(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""

    def _extract_raw_author(self, data: dict) -> str:
        """ì›ì‹œ ì‘ì„±ì ì •ë³´ ì¶”ì¶œ"""
        try:
            by_list = data.get("credits", {}).get("by", [])
            return by_list[0].get("name", "") if by_list else ""
        except Exception as e:
            logging.error(f"ì‘ì„±ì ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""

    def _clean_author(self, author: str) -> str:
        """ì‘ì„±ì ì´ë¦„ ì •ì œ (ê¸°ì, ê´„í˜¸ ë‚´ìš© ì œê±°)"""
        try:
            if not author:
                return ""
            author = re.sub(r'\s*ê¸°ì\s*\([^)]*\)', '', author)
            return super().clean_author(author).strip()
        except Exception as e:
            logging.error(f"ì‘ì„±ì ì •ë³´ ì •ì œ ì˜¤ë¥˜: {e}")
            return author or ""

    def _clean_body(self, text: str) -> str:
        """ë³¸ë¬¸ ì •ì œ (ğŸŒ ë§ˆì»¤ ì´í›„ ë‚´ìš© ì œê±°)"""
        try:
            if not text:
                return ""
            return re.split(r'\s*-\s*ğŸŒ', text, maxsplit=1)[0].strip()
        except Exception as e:
            logging.error(f"ë³¸ë¬¸ ì •ì œ ì˜¤ë¥˜: {e}")
            return text or ""
