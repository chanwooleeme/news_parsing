import os
import boto3
import logging
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime, timedelta
import time
import json
import random
from typing import Optional, List
import requests
from urllib.parse import urlparse
import io
import gc
import sys
from enum import Enum

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ê¸°ë³¸ì€ INFO)
LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    format="%(asctime)s [%(levelname)s] [%(name)s] %(message)s",
    level=LOG_LEVEL,
)
logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜ë¡œë¶€í„° SQS í URL ë°›ì•„ì˜¤ê¸°
FAILURE_QUEUE_URL: str = os.environ.get("FAILURE_QUEUE_URL", "https://sqs.ap-northeast-2.amazonaws.com/859805987151/MyTestDLQ")
RESUME_DATE_STR: Optional[str] = os.environ.get("RESUME_DATE")  # ì˜ˆ: "2025-02-15"
CATEGORY = os.environ.get("CATEGORY", "unknown")
TARGET_URL = os.environ.get("TARGET_URL")
XPATH_CHANGE_DATE = os.environ.get("XPATH_CHANGE_DATE")
URL_STORAGE_BUCKET = "article-crawl-html-storage"

class ErrorType(Enum):
    RETRY_ONCE = "RETRY_ONCE"
    RETRY_ALL = "RETRY_ALL"
    NEEDS_REVIEW = "NEEDS_REVIEW"
    S3 = "S3"
    HTML = "HTML"

# SQS í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ë¦¬ì „ ëª…ì‹œ)
sqs_client = boto3.client('sqs', region_name='ap-northeast-2')

# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
s3_client = boto3.client('s3', region_name='ap-northeast-2')

# === ê³µí†µ CSS ì…€ë ‰í„° ë° URL ì œì™¸ ì„¤ì • ===
CSS_PAGING_SELECTOR = "div#paging a.paging-num"
XPATH_PAGING_BUTTON_TEMPLATE = "//div[@id='paging']/a[text()='{page_number}']"
CSS_ARTICLE_LIST = "#recentList li"
CSS_ARTICLE_TITLE = "h2.tit a"
CSS_ARTICLE_BYLINE = ".byline"
CSS_ARTICLE_SUMMARY = ".lead"
EXCLUDE_URLS = [
    '/login', '/SecListData.html',
    '/national/national-general/article/201709021732001',
    '/national/national-general/article/201609241422011',
    '/article/201709021732001'
]

def send_failure_message(error_type: str, error: Exception, resume_date: str = None):
    message = {
        "category": CATEGORY,
        "error_type": error_type,
        "error": str(error),
        "resume_date": resume_date
    }
    sqs_client.send_message(QueueUrl=FAILURE_QUEUE_URL, MessageBody=json.dumps(message))
    logger.error(f"[{CATEGORY}] ì‹¤íŒ¨ íì— ì „ì†¡ | Type: {error_type} | Resume: {message['resume_date']}")

# === ë“œë¼ì´ë²„ ë° ë”œë ˆì´ ì„¤ì • í•¨ìˆ˜ ===
def get_chrome_options() -> Options:
    options = Options()
    options.add_argument('--headless=new')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_experimental_option('excludeSwitches', ['enable-automation'])
    options.add_experimental_option('useAutomationExtension', False)
    return options

def init_driver() -> webdriver.Chrome:
    try:
        driver = webdriver.Chrome(options=get_chrome_options())
        logger.info(f"[{CATEGORY}] ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
        return driver
    except Exception as e:
        send_failure_message(ErrorType.NEEDS_REVIEW, e)
        raise

def polite_delay(min_delay: float = 1.0, max_delay: float = 3.0) -> None:
    delay = random.uniform(min_delay, max_delay)
    logger.debug(f"ì ì‹œ ëŒ€ê¸°: {delay:.2f}ì´ˆ")
    time.sleep(delay)

# === ë‚ ì§œ ë³€ê²½ í•¨ìˆ˜ (ì˜¤ë¥˜ ë°œìƒ ì‹œ FailureQueueì— ë¡œê·¸ ì „ì†¡) ===
def change_date(driver: webdriver.Chrome, target_date: datetime, xpath: str) -> bool:
    try:
        btn = driver.find_element(By.XPATH, xpath)
        driver.execute_script("arguments[0].click();", btn)
        return True
    except Exception as e:
        logger.error(f"[{CATEGORY}] ë‚ ì§œ ë³€ê²½ ì‹¤íŒ¨ | {target_date.strftime('%Y-%m-%d')} | Error: {str(e)}")
        logger.error("íƒœìŠ¤í¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        send_failure_message(ErrorType.NEEDS_REVIEW, "f{CATEGORY}-{target_date.strftime('%Y-%m-%d')} ë‚ ì§œ ë³€ê²½ ì‹¤íŒ¨, í¬ë¡¤ë§ ì¢…ë£Œ.", target_date.strftime('%Y-%m-%d'))
        sys.exit(0)
        return False

# === í˜ì´ì§• ë° ê¸°ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ (ì˜¤ë¥˜ ë°œìƒ ì‹œ FailureQueueì— ë¡œê·¸ ì „ì†¡) ===
def extract_articles_from_page(driver: webdriver.Chrome, wait: WebDriverWait, current_date) -> List[str]:
    try:
        # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#recentList")))
        articles = container.find_elements(By.CSS_SELECTOR, "li")  # ì‹¤ì œ ê¸°ì‚¬ ìš”ì†Œ ì¡°íšŒ
        
        # ê¸°ì‚¬ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì •ìƒ ì²˜ë¦¬)
        if not articles:
            logger.info(f"[{CATEGORY}]-{current_date} ê¸°ì‚¬ 0ê±´ - ì •ìƒ ì²˜ë¦¬")
            
        return [article.find_element(By.CSS_SELECTOR, CSS_ARTICLE_TITLE).get_attribute("href") for article in articles]

    except Exception as e:
        # ì‹¤ì œ ì˜ˆì™¸ ë°œìƒì‹œì—ë§Œ ì—ëŸ¬ ë¡œê¹…
        logger.error(f"[{CATEGORY}]-{current_date} ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ | Error: {str(e)}", exc_info=True)
        send_failure_message(ErrorType.RETRY_ONCE, e, resume_date=current_date)
        return []

def get_total_pages(driver: webdriver.Chrome) -> int:
    try:
        paging_links = driver.find_elements(By.CSS_SELECTOR, CSS_PAGING_SELECTOR)
        if not paging_links:  # í˜ì´ì§€ ìš”ì†Œê°€ ì—†ëŠ” ê²½ìš° ë‹¨ì¼ í˜ì´ì§€ë¡œ ê°„ì£¼
            logger.info("í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œ ì—†ìŒ, ë‹¨ì¼ í˜ì´ì§€ ì²˜ë¦¬")
            return 1
        max_page = max([int(link.text) for link in paging_links if link.text.isdigit()])
        logger.info(f"í˜„ì¬ ìµœëŒ€ í˜ì´ì§€: {max_page}")
        return max_page
    except Exception as e:
        logger.warning(f"í˜ì´ì§€ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ë‹¨ì¼ í˜ì´ì§€ ì²˜ë¦¬: {str(e)}")
        return 1  # ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ 1 ë°˜í™˜

def go_to_page(driver: webdriver.Chrome, wait: WebDriverWait, page: int) -> bool:
    try:
        xpath = XPATH_PAGING_BUTTON_TEMPLATE.format(page_number=page)
        btn = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))
        driver.execute_script("arguments[0].click();", btn)
        return True
    except Exception as e:
        logger.error(f"[{CATEGORY}] í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ | ëª©í‘œ í˜ì´ì§€: {page} | Error: {str(e)}")
        return False

def simple_download_html(link: str, target_date: str) -> None:
    """ê°œì„ ëœ requests ë‹¤ìš´ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™” ë° S3 ì—°ë™)"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.google.com/',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    
    try:
        time.sleep(random.uniform(1, 2))
        parsed_url = urlparse(link)
        path_parts = parsed_url.path.split('/')
        article_number = path_parts[-1]
        filename = f"{article_number}.html"

        with requests.Session() as session:
            session.headers.update(headers)
            
            # ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
            with session.get(link, stream=True, timeout=10) as response:
                if response.status_code != 200:
                    error_msg = f"HTTP {response.status_code} ì‘ë‹µ"
                    logger.error(f"ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                    send_failure_message(ErrorType.S3, Exception(error_msg), target_date)
                    return

                # ì²­í¬ ë‹¨ìœ„ ì“°ê¸°ë¡œ ë©”ëª¨ë¦¬ ë¶€í•˜ ê°ì†Œ
                with open(filename, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            
                logger.info(f"ì„ì‹œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
                
                # íŒŒì¼ì—ì„œ ì§ì ‘ ì½ì–´ S3 ì—…ë¡œë“œ
                with open(filename, 'rb') as f:
                    html_content = f.read()
                    upload_to_s3(html_content, link, file_name=filename)
                
                # ì—…ë¡œë“œ ì„±ê³µ í›„ ì¦‰ì‹œ íŒŒì¼ ì‚­ì œ
                os.remove(filename)
                logger.info(f"S3 ì—…ë¡œë“œ í›„ íŒŒì¼ ì‚­ì œ: {filename}")

                # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
                del html_content
                gc.collect()

    except Exception as e:
        logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ | URL: {link} | ë‚ ì§œ: {target_date} | Error: {str(e)}", exc_info=True)
        send_failure_message("HTML_DOWNLOAD", e, target_date)
    finally:
        # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ íŒŒì¼ ì •ë¦¬
        if 'filename' in locals() and os.path.exists(filename):
            os.remove(filename)
            logger.warning(f"ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ ì¸í•œ íŒŒì¼ ì •ë¦¬: {filename}")

def upload_to_s3(html_content: bytes, url: str, file_name) -> bool:  # ë°˜í™˜ íƒ€ì… ì¶”ê°€
    try:
        s3_key = f"{CATEGORY}/{file_name}"

        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ
        with io.BytesIO(html_content) as buffer:
            s3_client.upload_fileobj(
                Fileobj=buffer,
                Bucket=URL_STORAGE_BUCKET,
                Key=s3_key,
                ExtraArgs={'ContentType': 'text/html'}
            )
        logger.info(f"S3 ì—…ë¡œë“œ ì„±ê³µ | Key: {s3_key}")
        return True  # ì„±ê³µ ì—¬ë¶€ ë°˜í™˜
    except Exception as e:
        logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨ | URL: {url} | Error: {str(e)}")
        send_failure_message(ErrorType.S3, e)
        return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜

def format_date(dt: datetime, include_time: bool = False) -> str:
    """datetimeì„ ë¬¸ìì—´ë¡œ ê°„ë‹¨ í¬ë§·íŒ…"""
    return dt.strftime('%Y-%m-%d %H:%M:%S') if include_time else dt.strftime('%Y-%m-%d')

# === ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜ ===
def crawl_category(config: dict) -> None:
    category = config.get('category', 'unknown')
    
    # ì¢…ë£Œ ë‚ ì§œë¥¼ ê°„ë‹¨í•œ ë‚ ì§œë¡œ ì„¤ì •
    END_DATE = datetime(2020, 1, 1)
    
    # íƒ€ê²Ÿ ë‚ ì§œ ì´ˆê¸°í™” (ì–´ì œ ë‚ ì§œ ê¸°ì¤€)
    base_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)
        
    logger.info(f"ğŸš€ [{category}] í¬ë¡¤ë§ ì‹œì‘ | ë²”ìœ„: {format_date(base_date)} ~ {format_date(END_DATE)}")

    driver = init_driver()
    wait = WebDriverWait(driver, 10)
    
    try:
        driver.get(config["url"])
        logger.info(f"[{category}] URL ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"[{category}] URL ë¡œë“œ ì‹¤íŒ¨ | Error: {str(e)}")
        driver.quit()
        return
    
    target_date = base_date

    if config.get('resume_date') != None and config.get('resume_date') != target_date:
        try:
            resume_date = config.get('resume_date')
            logger.info(f"{target_date}ë¥¼ {resume_date}ê¹Œì§€ ì´ë™í•©ë‹ˆë‹¤.")
            while target_date != resume_date:
                logger.info(f"í˜„ì¬ë‚ ì§œ: {target_date}, íƒ€ê²Ÿë‚ ì§œ: {resume_date}.")
                change_date(driver, resume_date, config["xpath_change_date"])
                target_date -= timedelta(days=1)
                polite_delay(0.5, 1)
        except Exception as e:
            send_failure_message("FAIL_AGAIN", e)
            logger.error(f"[{category}] ì´ë™ ì‹¤íŒ¨ | Error: {str(e)}")
            driver.quit()
    
        logger.info("f{target_date}ë¡œ ì´ë™ ì™„ë£Œ. í¬ë¡¤ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    while target_date >= END_DATE:
        current_date_str = format_date(target_date)
        logger.info(f"[{category}] í˜„ì¬ ì²˜ë¦¬ ë‚ ì§œ: {current_date_str}")
        
        # 3. ë‚ ì§œ ë³€ê²½ ì‹œë„ (ë‹¤ìŒ ë²„íŠ¼ í´ë¦­)
        if not change_date(driver, target_date, config["xpath_change_date"]):
            logger.warning(f"[{category}] ë‚ ì§œ ë³€ê²½ ì‹¤íŒ¨ | ê±´ë„ˆëœ€")
            target_date -= timedelta(days=1)
            continue

        # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        current_page = 1
        while True:
            total_pages = get_total_pages(driver)
            target_date_str = format_date(target_date)
            logger.info(f"[{category}][{target_date_str}] í˜ì´ì§€ ì§„í–‰ | {current_page}/{total_pages}")
            
            # ê¸°ì‚¬ ì¶”ì¶œ
            articles = extract_articles_from_page(driver, wait, target_date)
            
            if articles:
                for article in articles:
                    simple_download_html(article, target_date_str)
                logger.info(f"[{category}][{target_date_str}] {len(articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
            else:
                logger.warning(f"[{category}][{target_date_str}] {target_date_str} ê¸°ì‚¬ ì—†ìŒ")

            if current_page >= total_pages:
                logger.info(f"[{category}][{target_date_str}] ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                break
                
            if not go_to_page(driver, wait, current_page + 1):
                logger.warning(f"[{category}][{target_date_str}] í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨. í¬ë¡¤ë§ ì¤‘ë‹¨")
                break
                
            current_page += 1
            polite_delay(2, 3)

        # 4. ë‚ ì§œ ê°±ì‹  (ë£¨í”„ ì¢…ë£Œ ì¡°ê±´ í™•ì¸)
        target_date -= timedelta(days=1)
        if target_date < END_DATE:
            logger.info(f"[{category}] {format_date(END_DATE)} ë„ë‹¬. í¬ë¡¤ë§ ì¢…ë£Œ")
            break

    logger.info(f"[{category}] í¬ë¡¤ë§ ì™„ë£Œ")
    driver.quit()

def main() -> None:
    resume_date: Optional[datetime] = None
    if RESUME_DATE_STR:
        try:
            resume_date = datetime.strptime(RESUME_DATE_STR, "%Y-%m-%d")
        except Exception as e:
            logger.error(f"RESUME_DATE í˜•ì‹ ì˜¤ë¥˜: {e}")
    config = {
        "category": CATEGORY,
        "url": TARGET_URL,
        "xpath_change_date": XPATH_CHANGE_DATE,
        "resume_date": resume_date
    }
    crawl_category(config)

if __name__ == '__main__':
    if not all([TARGET_URL, XPATH_CHANGE_DATE]):  # URL_TASK_QUEUE_URL ê²€ì¦ ì œê±°
        logger.error("í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        exit(1)
        
    main()
    logger.info(f"=== [{CATEGORY}] í¬ë¡¤ë§ ì¢…ë£Œ ===")
