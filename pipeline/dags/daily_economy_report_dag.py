# /dags/daily_economy_report_dag.py

from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago
from datetime import timedelta
import os
import json
from datetime import datetime
from logger import get_logger

from agentic_retriever.preprocessor.economy_filter import EconomyFilter
from agentic_retriever.retriever.news_summarizer import NewsSummarizer
from agentic_retriever.retriever.summary_aggregator import SummaryAggregator
from agentic_retriever.evaluator.sufficiency_checker import SufficiencyChecker
from agentic_retriever.retriever.missing_info_analyzer import MissingInfoAnalyzer
from agentic_retriever.retriever.missing_info_searcher import MissingInfoSearcher
from agentic_retriever.report_generator.report_aggregator import ReportAggregator
from agentic_retriever.config.models import ModelName
from vector_store.vector_store import NewsVectorStore
from clients.qdrant_vector_store import QdrantVectorStore
from clients.slack_client import SlackClient

logger = get_logger(__name__)

default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def save_test_data(data, step_name, data_type="json"):
    """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = "/opt/airflow/data/test_outputs"
    os.makedirs(save_dir, exist_ok=True)
    
    filename = f"{save_dir}/{step_name}_{timestamp}"
    
    if data_type == "json":
        with open(f"{filename}.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    else:
        with open(f"{filename}.txt", "w", encoding="utf-8") as f:
            f.write(str(data))
    
    logger.info(f"ğŸ’¾ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥: {step_name} -> {filename}")

with DAG(
    dag_id='daily_economy_report_dag',
    default_args=default_args,
    start_date=days_ago(1), 
    schedule_interval="0 0 * * *",  # UTC 0ì‹œ (KST 9ì‹œ)
    catchup=False,
    tags=['news', 'report'],
) as dag:

    @task
    def generate_and_send_report():
        from utils.client import get_openai_client, get_qdrant_client
        from utils.economy_keyword import get_all_categories

        logger.info("ğŸ”„ ê²½ì œ ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        openai_client = get_openai_client()
        qdrant_client = get_qdrant_client()
        collection_name = "economy-articles"
        slack_token = os.getenv("SLACK_TOKEN")
        
        # 1. ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        logger.info("ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ ì‹œì‘")
        vs = QdrantVectorStore(qdrant_client, collection_name=collection_name)
        vector_store = NewsVectorStore(vector_store=vs)
        articles = vector_store.retrieve_news(
            economic_variables=get_all_categories(), recent_days=1, limit=100
        )
        logger.info(f"ğŸ“Š ì¡°íšŒëœ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜: {len(articles)}")
        save_test_data(articles, "raw_articles")

        # 2. ê²½ì œ ê´€ë ¨ í•„í„°ë§
        logger.info("ğŸ” ê²½ì œ ê´€ë ¨ ê¸°ì‚¬ í•„í„°ë§ ì‹œì‘")
        economy_filter = EconomyFilter(openai_client, model=ModelName.GPT_4O_MINI)
        economy_articles = economy_filter.filter_economy_articles(articles)
        logger.info(f"ğŸ“ˆ ê²½ì œ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜: {len(economy_articles)}")
        save_test_data(economy_articles, "filtered_economy_articles")

        # 3. ë‰´ìŠ¤ ìš”ì•½
        logger.info("ğŸ“ ë‰´ìŠ¤ ìš”ì•½ ì‹œì‘")
        summarizer = NewsSummarizer(openai_client, model=ModelName.GPT_4O_MINI)
        partial_summaries = summarizer.summarize_batches(economy_articles)
        logger.info(f"ğŸ“‹ ìƒì„±ëœ ë¶€ë¶„ ìš”ì•½ ìˆ˜: {len(partial_summaries)}")
        save_test_data(partial_summaries, "partial_summaries")

        # 4. ìš”ì•½ í•©ì¹˜ê¸°
        logger.info("ğŸ”„ ìš”ì•½ í†µí•© ì‹œì‘")
        aggregator = SummaryAggregator(openai_client, model=ModelName.GPT_4O_MINI)
        final_summary = aggregator.aggregate_summaries(partial_summaries)
        logger.info("âœ… ìš”ì•½ í†µí•© ì™„ë£Œ")
        save_test_data(final_summary, "final_summary", "txt")

        # 5. sufficiency í‰ê°€
        logger.info("ğŸ“Š ìš”ì•½ ì¶©ë¶„ì„± í‰ê°€ ì‹œì‘")
        checker = SufficiencyChecker(openai_client, model=ModelName.GPT_4O)
        is_insufficient, decisions = checker.check_sufficiency(final_summary)
        logger.info(f"ğŸ“Š ì¶©ë¶„ì„± í‰ê°€ ê²°ê³¼: {'ë¶€ì¡±' if is_insufficient else 'ì¶©ë¶„'}")
        save_test_data({
            "is_insufficient": is_insufficient,
            "decisions": decisions
        }, "sufficiency_check")

        # 6. ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰
        if is_insufficient:
            logger.info("ğŸ” ë¶€ì¡±í•œ ì •ë³´ ì¶”ê°€ ê²€ìƒ‰ ì‹œì‘")
            analyzer = MissingInfoAnalyzer(openai_client, model=ModelName.GPT_4O_MINI)
            missing_keywords = analyzer.analyze_missing_info(final_summary)
            logger.info(f"ğŸ”‘ ì¶”ê°€ ê²€ìƒ‰ í‚¤ì›Œë“œ: {missing_keywords}")
            save_test_data(missing_keywords, "missing_keywords")

            searcher = MissingInfoSearcher(vector_store)
            missing_articles = searcher.search_missing_info(missing_keywords)
            logger.info(f"ğŸ“° ì¶”ê°€ ê²€ìƒ‰ëœ ê¸°ì‚¬ ìˆ˜: {len(missing_articles)}")
            save_test_data(missing_articles, "missing_articles")

            # context í™•ì¥
            extended_articles = economy_articles + missing_articles
            partial_summaries = summarizer.summarize_batches(extended_articles)
            final_summary = aggregator.aggregate_summaries(partial_summaries)
            logger.info("âœ… ì¶”ê°€ ì •ë³´ í†µí•© ì™„ë£Œ")
            save_test_data(final_summary, "final_summary_after_extension", "txt")

        # 7. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        logger.info("ğŸ“„ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
        report_builder = ReportAggregator(openai_client, model=ModelName.GPT_4O)
        final_report = report_builder.generate_final_report(final_summary)
        logger.info("âœ… ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        save_test_data(final_report, "final_report", "txt")

        # 8. Slack ë°œì†¡
        logger.info("ğŸ“¤ Slack ë°œì†¡ ì‹œì‘")
        slack = SlackClient(slack_token)
        slack.send_message(channel="#economy-reports", text=final_report)
        logger.info("âœ… Slack ë°œì†¡ ì™„ë£Œ")
        logger.info("ğŸ‰ ê²½ì œ ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")

    generate_and_send_report()
