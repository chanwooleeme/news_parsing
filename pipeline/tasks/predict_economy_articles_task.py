from typing import List, Dict
from logging import getLogger
from utils.file import list_directories, list_files, join_path, read_json_file
import requests
import os

logger = getLogger(__name__)
MAX_BATCH_SIZE = 100
BASE_API_URL = os.getenv("BASE_API_URL")

# ğŸ“° ê¸°ì‚¬ ë¡œë”©
def load_articles_from_directory(base_dir: str) -> List[Dict]:
    articles = []

    for newspaper in list_directories(base_dir):
        newspaper_dir = join_path(base_dir, newspaper)
        for filename in list_files(newspaper_dir, extension=".json"):
            json_path = join_path(newspaper_dir, filename)
            try:
                article = read_json_file(json_path)
                content = article.get("content", "")

                if not content:
                    logger.warning(f"âŒ Content ë¹„ì–´ìˆìŒ: {json_path}")
                    continue
                elif len(content) <= 100:
                    logger.warning(f"âŒ Content ê¸¸ì´ ë„ˆë¬´ ì§§ì€ ê¸°ì‚¬: {json_path}")
                    continue

                articles.append(article)
            except Exception as e:
                logger.error(f"âŒ {newspaper}/{filename} ì½ê¸° ì‹¤íŒ¨: {e}")
    
    logger.info(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return articles

# ğŸ§© ë°°ì¹˜ ë‚˜ëˆ„ê¸°
def batch_articles(articles: List[Dict], batch_size: int = MAX_BATCH_SIZE) -> List[Dict]:
    return [
        {"articles": articles[i:i + batch_size]}
        for i in range(0, len(articles), batch_size)
    ]


# ğŸš€ ë™ê¸° API í˜¸ì¶œ (ìˆ˜ì •)
def call_predict(batch, idx): # async ì œê±°, session ì œê±°
    try:
        # aiohttp ëŒ€ì‹  requests ì‚¬ìš© (ìˆ˜ì •)s
        response = requests.post(BASE_API_URL + "/predict", json=batch, timeout=180) # íƒ€ì„ì•„ì›ƒ 3ë¶„ìœ¼ë¡œ ì„¤ì •
        logger.info(f"âœ… Batch {idx} ì‘ë‹µ ì½”ë“œ: {response.status_code}") # resp.status -> response.status_code
        # ì‘ë‹µ ìƒíƒœ ì½”ë“œ í™•ì¸ ì¶”ê°€
        if response.status_code != 200:
             logger.error(f"âŒ Batch {idx} ì²˜ë¦¬ ì‹¤íŒ¨: Status {response.status_code}, Response: {response.text}")
    except requests.exceptions.RequestException as e: # aiohttp ì˜ˆì™¸ ëŒ€ì‹  requests ì˜ˆì™¸ ì²˜ë¦¬ (ìˆ˜ì •)
        logger.error(f"âŒ Batch {idx} ìš”ì²­ ì‹¤íŒ¨: {e}")
    except Exception as e: # ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
        logger.error(f"âŒ Batch {idx} ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")


def predict_all_batches(batches): # async ì œê±° (ìˆ˜ì •)
    # asyncio.gather ëŒ€ì‹  ë‹¨ìˆœ for ë£¨í”„ ì‚¬ìš© (ìˆ˜ì •)
    for idx, batch in enumerate(batches):
        call_predict(batch, idx + 1) 

# ğŸ¯ ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜
def predict_economy_articles_task(parsed_dir: str):
    logger.info("API URL: " + BASE_API_URL + "/predict")
    articles = load_articles_from_directory(parsed_dir)
    batches = batch_articles(articles)

    logger.info(f"ğŸš€ ì´ {len(batches)}ê°œ ë°°ì¹˜ ë™ê¸° ìš”ì²­ ì‹œì‘")      
    predict_all_batches(batches)
    logger.info("âœ… ëª¨ë“  ê¸°ì‚¬ ì˜ˆì¸¡ ì™„ë£Œ")
